[
  {
    "question": "Your company has developed a monolithic, 3-tier application to allow external users to upload and share files. The solution cannot be easily enhanced and lacks reliability. The development team would like to re-architect the application to adopt microservices and a fully managed service approach, but they need to convince their leadership that the effort is worthwhile. Which advantage(s) should they highlight to leadership?",
    "code": "",
    "multichoice": false,
    "alternatives": {
      "A": {
        "answer": "The new approach will be significantly less costly, make it easier to manage the underlying infrastructure, and automatically manage the CI/CD pipelines.",
        "correct": false,
        "why": ""
      },
      "B": {
        "answer": "The monolithic solution can be converted to a container with Docker. The generated container can then be deployed into a Kubernetes cluster.",
        "correct": false,
        "why": ""
      },
      "C": {
        "answer": "The new approach will make it easier to decouple infrastructure from application, develop and release new features, manage the underlying infrastructure, manage CI/CD pipelines and perform A/B testing, and scale the solution if necessary.",
        "correct": true,
        "why": ""
      },
      "D": {
        "answer": "The process can be automated with Migrate for Compute Engine.",
        "correct": false,
        "why": ""
      }
    }
  },
  {
    "question": "Your team is developing a web application that will be deployed on Google Kubernetes Engine (GKE). Your CTO expects a successful launch and you need to ensure your application can handle the expected load of tens of thousands of users. You want to test the current deployment to ensure the latency of your application stays below a certain threshold. What should you do?",
    "code": "",
    "multichoice": false,
    "alternatives": {
      "A": {
        "answer": "Use a load testing tool to simulate the expected number of concurrent users and total requests to your application, and inspect the results.",
        "correct": true,
        "why": ""
      },
      "B": {
        "answer": "Enable autoscaling on the GKE cluster and enable horizontal pod autoscaling on your application deployments. Send curl requests to your application, and validate if the auto scaling works.",
        "correct": false,
        "why": ""
      },
      "C": {
        "answer": "Replicate the application over multiple GKE clusters in every Google Cloud region. Configure a global HTTP(S) load balancer to expose the different clusters over a single global IP address.",
        "correct": false,
        "why": ""
      },
      "D": {
        "answer": "Use Cloud Debugger in the development environment to understand the latency between the different microservices.",
        "correct": false,
        "why": ""
      }
    }
  },
  {
    "question": "Your company has a Kubernetes application that pulls messages from Pub/Sub and stores them in Filestore. Because the application is simple, it was deployed as a single pod. The infrastructure team has analyzed Pub/Sub metrics and discovered that the application cannot process the messages in real time. Most of them wait for minutes before being processed. You need to scale the elaboration process that is I/O-intensive. What should you do?",
    "code": "",
    "multichoice": false,
    "alternatives": {
      "A": {
        "answer": "Use kubectl autoscale deployment APP_NAME --max 6 --min 2 --cpu-percent 50 to configure Kubernetes autoscaling deployment.",
        "correct": false,
        "why": ""
      },
      "B": {
        "answer": "Configure a Kubernetes autoscaling deployment based on the subscription/push_request_latencies metric.",
        "correct": false,
        "why": ""
      },
      "C": {
        "answer": "Use the --enable-autoscaling flag when you create the Kubernetes cluster.",
        "correct": false,
        "why": ""
      },
      "D": {
        "answer": "Configure a Kubernetes autoscaling deployment based on the subscription/num_undelivered_messages metric.",
        "correct": true,
        "why": ""
      }
    }
  },
  {
    "question": "Your company is developing a web-based application. You need to make sure that production deployments are linked to source code commits and are fully auditable. What should you do?",
    "code": "",
    "multichoice": false,
    "alternatives": {
      "A": {
        "answer": "Make sure a developer is tagging the code commit with the date and time of commit.",
        "correct": false,
        "why": ""
      },
      "B": {
        "answer": "Make sure a developer is adding a comment to the commit that links to the deployment.",
        "correct": false,
        "why": ""
      },
      "C": {
        "answer": "Make the container tag match the source code commit hash.",
        "correct": true,
        "why": ""
      },
      "D": {
        "answer": "Make sure the developer is tagging the commits with latest.",
        "correct": false,
        "why": ""
      }
    }
  },
  {
    "question": "An application development team has come to you for advice. They are planning to write and deploy an HTTP(S) API using Go 1.12. The API will have a very unpredictable workload and must remain reliable during peaks in traffic. They want to minimize operational overhead for this application. Which approach should you recommend?",
    "code": "",
    "multichoice": false,
    "alternatives": {
      "A": {
        "answer": "Develop the application with containers, and deploy to Google Kubernetes Engine.",
        "correct": false,
        "why": ""
      },
      "B": {
        "answer": "Develop the application for App Engine standard environment.",
        "correct": true,
        "why": ""
      },
      "C": {
        "answer": "Use a Managed Instance Group when deploying to Compute Engine.",
        "correct": false,
        "why": ""
      },
      "D": {
        "answer": "Develop the application for App Engine flexible environment, using a custom runtime.",
        "correct": false,
        "why": ""
      }
    }
  },
  {
    "question": "Your company is designing its data lake on Google Cloud and wants to develop different ingestion pipelines to collect unstructured data from different sources.After the data is stored in Google Cloud, it will be processed in several data pipelines to build a recommendation engine for end users on the website. The structure of the data retrieved from the source systems can change at any time. The data must be stored exactly as it was retrieved for reprocessing purposes in case the data structure is incompatible with the current processing pipelines. You need to design an architecture to support the use case after you retrieve the data. What should you do?",
    "code": "",
    "multichoice": false,
    "alternatives": {
      "A": {
        "answer": "Send the data through the processing pipeline, and then store the processed data in a BigQuery table for reprocessing.",
        "correct": false,
        "why": ""
      },
      "B": {
        "answer": "Store the data in a BigQuery table. Design the processing pipelines to retrieve the data from the table.",
        "correct": false,
        "why": ""
      },
      "C": {
        "answer": "Send the data through the processing pipeline, and then store the processed data in a Cloud Storage bucket for reprocessing.",
        "correct": false,
        "why": ""
      },
      "D": {
        "answer": "Store the data in a Cloud Storage bucket. Design the processing pipelines to retrieve the data from the bucket.",
        "correct": true,
        "why": ""
      }
    }
  },
  {
    "question": "You are responsible for the Google Cloud environment in your company. Multiple departments need access to their own projects, and the members within each department will have the same project responsibilities. You want to structure your Google Cloud environment for minimal maintenance and maximum overview ofIAM permissions as each department's projects start and end. You want to follow Google-recommended practices. What should you do?",
    "code": "",
    "multichoice": false,
    "alternatives": {
      "A": {
        "answer": "Grant all department members the required IAM permissions for their respective projects.",
        "correct": false,
        "why": ""
      },
      "B": {
        "answer": "Create a Google Group per department and add all department members to their respective groups. Create a folder per department and grant the respective group the required IAM permissions at the folder level. Add the projects under the respective folders.",
        "correct": true,
        "why": ""
      },
      "C": {
        "answer": "Create a folder per department and grant the respective members of the department the required IAM permissions at the folder level. Structure all projects for each department under the respective folders.",
        "correct": false,
        "why": ""
      },
      "D": {
        "answer": "Create a Google Group per department and add all department members to their respective groups. Grant each group the required IAM permissions for their respective projects.",
        "correct": false,
        "why": ""
      }
    }
  },
  {
    "question": "Your company has an application running as a Deployment in a Google Kubernetes Engine (GKE) cluster. You have separate clusters for development, staging, and production. You have discovered that the team is able to deploy a Docker image to the production cluster without first testing the deployment in development and then staging. You want to allow the team to have autonomy but want to prevent this from happening. You want a Google Cloud solution that can be implemented quickly with minimal effort. What should you do?",
    "code": "",
    "multichoice": false,
    "alternatives": {
      "A": {
        "answer": "Configure a Kubernetes lifecycle hook to prevent the container from starting if it is not approved for usage in the given environment.",
        "correct": false,
        "why": ""
      },
      "B": {
        "answer": "Implement a corporate policy to prevent teams from deploying Docker images to an environment unless the Docker image was tested in an earlier environment.",
        "correct": false,
        "why": ""
      },
      "C": {
        "answer": "Configure binary authorization policies for the development, staging, and production clusters. Create attestations as part of the continuous integration pipeline.",
        "correct": true,
        "why": ""
      },
      "D": {
        "answer": "Create a Kubernetes admissions controller to prevent the container from starting if it is not approved for usage in the given environment.",
        "correct": false,
        "why": ""
      }
    }
  },
  {
    "question": "Your company wants to migrate their 10-TB on-premises database export into Cloud Storage. You want to minimize the time it takes to complete this activity, the overall cost, and database load. The bandwidth between the on-premises environment and Google Cloud is 1 Gbps. You want to follow Google-recommended practices. What should you do?",
    "code": "",
    "multichoice": false,
    "alternatives": {
      "A": {
        "answer": "Develop a Dataflow job to read data directly from the database and write it into Cloud Storage.",
        "correct": false,
        "why": ""
      },
      "B": {
        "answer": "Use the Data Transfer appliance to perform an offline migration.",
        "correct": false,
        "why": ""
      },
      "C": {
        "answer": "Use a commercial partner ETL solution to extract the data from the on-premises database and upload it into Cloud Storage.",
        "correct": false,
        "why": ""
      },
      "D": {
        "answer": "Compress the data and upload it with gsutil -m to enable multi-threaded copy.",
        "correct": true,
        "why": ""
      }
    }
  },
  {
    "question": "Your company has an enterprise application running on Compute Engine that requires high availability and high performance. The application has been deployed on two instances in two zones in the same region in active-passive mode. The application writes data to a persistent disk. In the case of a single zone outage, that data should be immediately made available to the other instance in the other zone. You want to maximize performance while minimizing downtime and data loss. What should you do?",
    "code": "",
    "multichoice": false,
    "alternatives": {
      "A": {
        "answer": "1. Attach a persistent SSD disk to the first instance. 2. Create a snapshot every hour. 3. In case of a zone outage, recreate a persistent SSD disk in the second instance where data is coming from the created snapshot.",
        "correct": false,
        "why": ""
      },
      "B": {
        "answer": "1. Create a Cloud Storage bucket. 2. Mount the bucket into the first instance with gcs-fuse. 3. In case of a zone outage, mount the Cloud Storage bucket to the second instance with gcs-fuse.",
        "correct": false,
        "why": ""
      },
      "C": {
        "answer": "1. Attach a regional SSD persistent disk to the first instance. 2. In case of a zone outage, force-attach the disk to the other instance.",
        "correct": true,
        "why": ""
      },
      "D": {
        "answer": "1. Attach a local SSD to the first instance disk. 2. Execute an rsync command every hour where the target is a persistent SSD disk attached to the second instance. 3. In case of a zone outage, use the second instance.",
        "correct": false,
        "why": ""
      }
    }
  }
]